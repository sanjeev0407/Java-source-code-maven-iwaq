Docker installation:
yum update && upgrade
curl https://get.docker.com/ | bash
docker info
docker --version

What is the container runtime in Kubernetes?:
The container runtime is the software that is responsible for running containers. 
Kubernetes supports container runtimes such as containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface).

Before installing need to pre-requsties:
sudo su - 
disbaling the Swap memory temporary:
swapoff -a
for permanent disabling swap memory:
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
cat /etc/fstab

Add kernel settings & Enable IP tables(CNI prerequisities):
Forwarding IPv4 and letting iptables see bridged traffic:
Execute the below mentioned instructions:
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

sysctl params required by setup, params persist across reboots:
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot:
sudo sysctl --system

Verify that the br_netfilter, overlay modules: 
lsmod | grep br_netfilter
lsmod | grep overlay 

Update the apt package index and install packages to allow apt to use a repository over HTTPS:
apt-get update -y 
sudo apt-get install ca-certificates curl gnupg lsb-release -y 

Add Docker’s official GPG key:
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

Use the following command to set up the repository:
echo \
"deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
"$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
 
Update the apt package index:
sudo apt-get update -y 
apt-get install containerd.io -y

generate default configuration file for containerd:
containerd config default> /etc/containerd/config.toml
cat /etc/containerd/config.toml

sed -i 's/SystemdCgroup \= false/SytemdCgroup \= true/g' /etc/containerd/config.toml
                (OR)
vi /etc/containerd/config.toml
Make it: SystemdCgroup = true

If you apply this change, make sure to restart containerd:
sudo systemctl restart containerd
sudo systemctl enable containerd
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------------------------------------------
Installing kubeadm, kubelet and kubectl:
Update the apt package index:
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl

Download the Google Cloud public signing key:
curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-archive-keyring.gpg

Add the Kubernetes apt repository:
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

Update apt package index, install kubelet, kubeadm and kubectl, and pin their version:
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl (only required in master node)
sudo apt-get install -y kubelet kubeadm -y (install in worker node)

apt-mark hold will prevent the package from being automatically upgraded or removed:
apt-mark hold kubelet kubeadm kubectl

enable and start kubelet service:
systemctl daemon-reload
systemctl start kubelet
systemctl enable kubelet.service

Kubeadm: helps as to Intilization the control plane
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Initialize K8s master:
kubeadm init

Move to Normal user :
Configure kubectl exit as root user & execute as normal ubuntu user:
exit

kubectl version --client

#Multi Master setup purpose we need to use it.
kubeadm init --control-plane-endpoint "PUBLIC_IP:PORT"

Your Kubernetes control-plane has initialized successfully!
 
To start using your cluster, you need to run the following as a regular user:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get nodes
kubectl get pods

You should now deploy a Pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node as root:

kubeadm join <control-plane-host>:<control-plane-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>

kubectl version --client
kubectl get nodes
kubectl get pods
kubectl get nodes -v=8
kubectl get pods -n kube-system -o wide

Install add-Ons: weavenet or Calico
This netwok drive communicate one pod to another pod within cluster with help of calico or weavent
Before installing Weave Net : Open the firewall ports TCP 6783, UDP 6783/6784
 
Install weavenet network in masternode:
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml 

Install Calico network masternode:
kubectl apply -f https://docs.projectcalico.org/archive/v3.20/manifests/calico.yaml

if it is calico need to add 6443 port into security inbound groups

Get Token for worker node: when you get the output you can execute the command in worker nodes
kubeadm token create --print-join-command
___________________________________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________________________________

K8S API resources /Objects:
Namespace
PodReplication controller
Replication controller
Replicaset
DaemonSet
Stateful setDeployment

Configurations:
ConfigMap
Secrets

To communicate the pods with in the cluster or outside the cluster we may need  few K8S  resource like:
Service
Network Policies
Ingress

Storage:
Presisstent Voulme
PersistentVolume Claim
StorageClasses

Horizantal POD AutoScaler

Role:
RoleBinding
CLusterRole
ClusterRole Binding
ServiceAccount

kubectl api-resources    --> this command need to check Api version of resources

Basic Commands:
kubectl get pods
kubectl get pods -v=8  i.e., V: Verbose mode

kubectl get all -v=8
kubectl get nodes
kubectl get deployments
kubectl get statefulsets
kubectl get Configmap
kubectl get DaemonSet
kubectl get namespaces
kubectl get  replicaset
kubectl get all
kubectl  get secrets

kubectl api-resources --namespaced=false    ---> this are cluster level resource
kubectl api-resources --namespaced=true     ---> this are isolated of resources namespace

Nodes:
Kubernetes runs your workload by placing containers into Pods to run on Nodes.
A node may be a virtual or physical machine, depending on the cluster.Each node is managed by the control plane and contains the services necessary to run Pods.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------
Namespaces:
In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster. 
Names of resources need to be unique within a namespace, but not across namespaces. 
Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc)

Default Namespaces:
Default 
kube-node-lease
kube-public
kube-system

kubectl get all -v=8
kubectl get all -n kube-system

uses Namespace:
Isolation: several teams working on same cluster , you can use namespaces to create separation between projects and microservices.
Permissions: Separate RBABC rues for each namespace, ensuring that only authorized roles can acces the resources in tht namespace.
Resource control: you can define resource limits at the namespace level, ensuring each namespace has access to a certain amount of CPU and memory resources.
                   This is enables separting cluster resources among several projects and ensuring each project has the rsources it needs.
performances: K8s API provides beteer performance when you define namespaces in the cluster. creating namespace can be done with a single command.

We need to create namespace two ways
Imperative: Imperative code focuses on writing an explicit sequence of commands to describe how you want the computer to do things
eg:    kubetl create namespace [NS]
       kubectl label namespace <NamespaceName> <Key>=<Value>
       kubectl run <PODName> --image=<Image> --port=<ContainerPort>
       kubectl create deployment <DeploymentName> --image <ImageName>

declarative: declarative code focuses on specifying the result of what you want.
eg:   
   apiVersion: v1
    kind: Namespace
    metadata:
        name: test-ns
   labels:
    name: test
    
Kubectl apply -f test-ns.yaml

For Declarative way main attribute components:
apiVersion: v1
kind: Namespace
metadata:
    name:
    labels:
        team: test
Spec:


kubectl apply vs kubectl create?:
Those are two different approaches:
Imperative Management:
kubectl create is what we call Imperative Management. 
On this approach you tell the Kubernetes API what you want to create, replace or delete, not how you want your K8s cluster world to look like.

Declarative Management:
kubectl apply is part of the Declarative Management approach, 
where changes that you may have applied to a live object (i.e. through scale) are "maintained" even if you apply other changes to the object.
---------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------------------
Pods:
Pods always runs on Node.
A pod is the smallest building block or basic unit od scheduling in Kubernetes
Pod is a group of one or more containers which will be running on the some node.

How containers communicate with each other within same pod:
Shared network namespace: All containers can comunicate with each other on the localhost.
shared storage Volumes: Multi containers in a pod can have the same volume mounted so that they can communicate with each other by reading and modifying the files in storage volume and using localhost(Shared network).


kubectl get Pods
kubectl get all -n test-ns -o wide
kubectl describe pod [podName]
kubectl describe pod [podName] -n test-ns
kubectl get nodes -o wide
curl -v http://[podIP]:[POrtNmber]/contextpath

kubectl get pods -o wide --show-labels -n test-ns     ---> For checking labels command

Within cluster able to access in any node
kubectl run pythonapp --image [ImageName] --port:8080 --labels "app-pythonapp" -n test-ns

ImageName: taken from dockerHub

kubectl get events -n test-ns

Logs:
kubectl logs [PodName] -n [NamespaceName]

if you want see containers inside of pod using below command:
kubectl exec [POdName] -n test-ns --ls -lar 

if you want see content inside of container within pod:
kubectl exec [PODName] -c [CN] --ls -lar

Logs inside container:
kubectl logs [PODName] -c [CN]

Enter into pod inside container:
kubectl exec -it [podName] -c [CN] -- bash 
if you get any error by using above command you can use below command:
kubectl exec -it [podName] -c [CN] -- sh

kubectl exec -it [POdName] -n test-ns -- bash
curl -v http://[PodIP]:PortNumber/api/tasks 

Default Place of K8S Manifests: cat /etc/Kubernetes/manifests/      --> when Intilization of kubeadm it's automatically created


What is StaticPOD:
In Kubernetes, a static pod is a special type of pod that is managed directly by the kubelet on a specific node, rather than by the Kubernetes control plane. 
It is defined as a pod manifest file placed on the node's local filesystem, and the kubelet monitors and manages the pod's lifecycle.Can't control by API server.

cat /etc/Kubernetes/manifests/

Here's how static pods work: 
The kubelet on a node periodically scans a specific directory (typically /etc/kubernetes/manifests) for static pod manifest files. Each manifest file represents a static pod.
When the kubelet detects a new manifest file, it creates and manages a pod based on the specifications in the file.
The kubelet monitors the pod's health and takes appropriate actions to maintain its desired state. It restarts the pod if it fails, handles scaling, etc.
Since static pods are not managed by the control plane, changes to their configurations (e.g., updating the manifest file) require manual intervention on the node where they are running.
If the kubelet on a node fails or is restarted, it automatically starts all the static pods defined on that node.

ReplicationController:
ReplicationController is act as LoadBalancer, It is used for High Avaliablity;
Selectors are not mandtory for replication controller
RC supports only Equality (Equal condition) based selectors
 Key == Value

Syntax:
Selector:
    <Key>: <Value>

kubectl delete rc [RC Name] -n [Namespace]
kubectl describe rs [ReplicaSetName]

ReplicaSet: Replicaset is the next generation of ReplicationController; The only difference between them Replication controller  having Selector Support.

It supports Equality Based Selectors & set Based (Expression like in or not in) Selectors 

Syntax:
selectors:
    Matchlabels: # Equality based selector in RS
    <Key>: <Value>
    
syntax:
selectors:
    MatchlExpressions: # set based selector in RS
    - <Key>: <PodLabelKey>
        Operator: in
        Values:
        - <PODLabelValue>: <POD>
______________________________________________________________________________________________________
______________________________________________________________________________________________________
DaemonSet: A Daemonset make sure that all or some Kubernetes nodes run a copy of a POD

Kubectl describe node [Ip-address]  --> Findout Tiants
kubectl get Daemonset [DS Name] -o yaml -n kube-system or [NSName]
______________________________________________________________________________________________________
______________________________________________________________________________________________________

Labels & Selectors:
Labels are key/value  pairs attached to object
you can make your own and apply It
it's like tag things in Kubernetes
E.g:
labels:
    app: nginx
    role: web
    env: Dev

Selectors:

Selectors us the label key to find a collection of obejcts  matched with same Value
it's like filter, conditions and query to your labels

______________________________________________________________________________________________________
______________________________________________________________________________________________________
Deployment:
Etcd maintain all deployment info
Rolling update is default strategy:

Deployment creates the replicaset, Replica set will be create pods, pods will be create containers.

deployment strategeis:
Rolling deployment:
Recreate:

kubectl rollout history deployment [DepoyName]
kubectl rollout history deployment [DepoyName] -revision 2
kubectl rollout history deployment [DepoyName] -n [NameSpace] -revision 2
kubectl apply -f [deployment.yaml] --record=true
kubectl rollout undo deployment [ReplicaSet] --to-revision 2

watch kubectl get pods

Deployment Techinc: Without downtime we need to update it.
Blue Green Deployment:
Blue: Running the current applciation version & Green: it is running new applciation version
Canaray Deployment:
Canary Deployment is a deployment strategy used in Kubernetes to gradually roll out changes to a new version of an application or service. 
The approach involves deploying the new version to a small subset of users or nodes (known as the "canary group") while keeping the remaining users or nodes on the previous version (known as the "baseline group"). 
This allows for monitoring and testing of the new version's performance, stability, and user experience before fully rolling it out.

______________________________________________________________________________________________________
______________________________________________________________________________________________________
Resource Management: (CPU, Memory for POds and containers):
CPU and memory are each a resource type. A resource type has a base unit. 
CPU represents compute processing and is specified in units of Kubernetes CPUs. Memory is specified in units of bytes.

When you specify the resource request for containers in a Pod, the kube-scheduler uses this information to decide which node to place the Pod on.

1cpu =1000m
0.5 cpu= 500m
0.25 cpu = 250m

m=Milicore

Mi=Memory
1Gi= 1024Mi

Syntax:
resources:
    requests:
       cpu:
       memory:
    limits:
       cpu:
       memory:

Requests and limits:
If the node where a Pod is running has enough of a resource available, 
it's possible (and allowed) for a container to use more resource than its request for that resource specifies. 
However, a container is not allowed to use more than its resource limit.

Limits should be always greater than or equals  to request

Resource types:
CPU and memory are each a resource type. A resource type has a base unit. 
CPU represents compute processing and is specified in units of Kubernetes CPUs. 
Memory is specified in units of bytes.

kubectl get nodes
kubectl describe [nodeIP]
kubectl get all -o wide
kubectl get ep
OOM Killed means: Out Of Memory Killed

Note: If you specify a limit for a resource, but do not specify any request, and no admission-time mechanism has applied a default request for that resource, 
then Kubernetes copies the limit you specified and uses it as the requested value for the resource.

if you not specify the requests also it's default to take limit range; but if you are giving the request range but it's not taking limit range.

______________________________________________________________________________________________________
______________________________________________________________________________________________________
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
When you give repicas count low or high ; it's Manual scalling

HorizontalPodAutoScaler(HPA):
HPA is implemented as a K8S API resource and controller. The resource determines the behaviour of the controller.
HPA is interact with Metric API to identify CPU/Memory utilization of POD.

Increase or descrease pod replicas:
Scale out Means increasing the number of replicas
Scale In Means decreasing the number of replicas

HPA and METRIC SERVER:
HPA
The Horizontal Pod Auto scaler automatically scales the number of pods in a replication controller, deployment, replica set based on observed CPU utilization or memory utilization.
The Horizontal Pod Auto scaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU/memory utilization to the target specified by user.
HPA will interact with Metric API to identify CPU/Memory Utilization of POD.

pre-requsties:
Kubernetes Metric Server:
Metric Server is an application that collects metrics from objects such as pods, nodes according to the state of CPU, RAM and keeps them in time.
Metric-Server can be installed in the system as an addon. You can take and install it directly from the repo.

===Setup Metrics Server===
Git Clone below repo in Kubernetes Master or in kubectl client Machine.
$ git clone https://github.com/Mithun TechnologiesDevOps/metrics-server.git
$ cd metrics-server
$ kubectl apply -f deploy/1.8+/


HPA has an algorithm that looks like this:
desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]

What does this mean? Let’s break it down: !
desiredReplicas: Replica count that will be sent to the controller after calculations.
ceil(): This is a function that rounds a fractional number upwards. For example ceil(12.18) is 13.
currentReplicas: Current number of pods for a given deployment or any other superset of “scale” object type.
currentMetricValue: Current value of metric for a given scaling factor metric. Can be 800m or 1.5Gi, for custom metrics it can be 500 events per second etc.
desiredMetricValue: Metric that has been set to comply by HPA. Eventually with all mechanisms HPA provides, your app runs at this metric value. 
                    This value should not be too low or too high.
                    
Kubectl get hpa 
kubectl get hpa -A 

For need to check service internally working or Not:
curl -v http://[IPAdrees of Application]

Mointoing CPU utilization of PODS:
watch kubectl top pods

This is for generating load on the Pod server:
kubectl run loadgenerator -it --image=busybox --rm --sh
wget -q0- http://php-apache

while true; do wget -q0- http://php-apache; done

Questions:
The HPA scaling is against request or limits: Based on Pod requests 
In deployment you mention 3 replicas; but HPA you mentioned min. 3 Replicaset: It's create 3 pods it's scale into 2 pods in HPA
______________________________________________________________________________________________________
______________________________________________________________________________________________________

Statefull & Stateless Application:
Suppose if pod is failed data will be lost so that's need to maintain volumes in the k8s

Very Dangerous command: WIthin namespace need to delete all applications
kubectl delete all --all -n test-ns

check logs:
kubectl logs [PoDName] -n test-ns

communication with DB and application:
kubectl exec [podName] -n test-ns --ls /data/db 

#Enter into the Application
kubectl exec -it [podName] -n test-ns --bash
# Most of the data saved in DB in /data folder
#after entering into the database
mongosh --host localhost:[PortNumber] -u username -p password
show databses;
use users
show collections
db.users.find()

______________________________________________________________________________________________________
______________________________________________________________________________________________________
Volumes:

Kubernetes supports many types of volumes. A Pod can use any number of volume types simultaneously.
Ephemeral volume: types have a lifetime of a pod, but persistent volumes exist beyond the lifetime of a pod.

Types:
hostPath
emptyDir
NFS
aws ElasticBlockStore
azureFile
azureDisk
googlePeristentDisk
ceph
glusterfs
configMap
persistentVolumeClaim
PersistentVolumes

Questions:

What is EmptyDir?: It is temporary storage; emptyDir is volume is initially empty. ;
It is first created when a pod is assigned to a node, and exists as long as that pod is running on that node.
Note: A container crashing does not remove a Pod from a node. The data in an emptyDir volume is safe across container crashes.

HostPath: HostPath volumes present many security risks, and it is a best practice to avoid the use of HostPaths when possible. 
When a HostPath volume must be used, it should be scoped to only the required file or directory, and mounted as ReadOnly.

Note: hostpath is not provide data consistency.(Ex: pod is running in one node with specfic data, but pod accidently failed/deleted, pod regenerate but pod is running on other node, In this time data will store in another node host path)

NFS(Network FIle System): Port number: 2049
nfs volume allows an existing NFS (Network File System) share to be mounted into a Pod. Unlike emptyDir, 
which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted.

Note: you must have your own NFS server running with the share exported before you can use it.

Setup NFS server:  for these create separate instance do below steps
sudo apt update -y
sudo apt install nfs-kernel-server -y 
sudo mkdir -p /mnt/nfs_share
sudo chown -R nobodu:nogroup -R /mnt/nfs_Share
sudo chmod 777 -R /mnt/nfs_share
sudo vi /etc/exports
/mnt/nfs_share *(rw, sync, no_subtree_check, no_root_squash)   --# any system can access with permissions after that save it :wq

sudo exportfs -a 
sudo systemctl restart nfs-kernel-server

#Below command(NFS client software) need to install in all master and worker nodes
sudo apt install nfs-common -y [OR] redhat server --> sudo apt install nfs-utilies -y

  volumes:
  - name: test-volume
    nfs:
      server: my-nfs-server.example.com/ NFS server Pvt.IP address
      path: /mnt/nfs_Share
      readOnly: true

PersistentVolume(CLuster level resource), persistentVolumeClaim(namespace resource): To manage the storage also by using PV, PVC resources
kubectl get PV
kubectl get PVC
kubectl get pvc -A 

Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides provides an API for users and administrators

PersistentVolume (PV): It is a piece of storage in the cluster that has been provisioned by an administrator Manually(Static volumes) or dynamically(Dynamic volumes)provisioned using Storage Classes.
                       PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV.
                       This API object(PersistentVolume)captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.
                       PV types are implemented as plugins.
                       
PersistentVolumeClaim (PVC): It is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources
                        Pods can request specific levels of resources (CPU and Memory). 
                        Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes).

PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource.
The interaction between PVs and PVCs follows this lifecycle:
Provisioning:
There are two ways PVs may be provisioned: statically or dynamically

Static:
A cluster administrator creates a number of PVs. 
They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption

Dynamic:
When none of the static PVs the administrator created match a user's PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. 
This provisioning is based on StorageClasses: the PVC must request a storage class and the administrator must have created and configured that class for dynamic provisioning to occur. 
Claims that request the class "" effectively disable dynamic provisioning for themselves.

Binding:
A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. 
A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together

How PV associte with binded PVC?:
Based on class, storage, requests

Access Modes :
A PersistentVolume can be mounted on a host in any way supported by the resource provider.
providers will have different capabilities and each PV's access modes are set to the specific modes supported by that particular volume.

ReadWriteOnce (RWO): The volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access the volume when the pods are running on the same node.
ReadOnlyMany (ROX): The volume can be mounted as read-only by many nodes.
ReadWriteMany (RWX): The volume can be mounted as read-write by many nodes.
ReadWriteOncePod (RWOP): The volume can be mounted as read-write by a single Pod. This is only supported for CSI volumes and Kubernetes version 1.22+.

Reference: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes

Reclaim Policies:
Retain:  When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered "released". 
         But it is not yet available for another claim because the previous claimant's data remains on the volume. 
ReCycle: For volume plugins that support the Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, 
         as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume
delete: If supported by the underlying volume plugin, the Recycle reclaim policy performs a basic scrub (rm -rf /thevolume/*) on the volume and makes it available again for a new claim.

kubectl describe pvc [pvc name] -n [namespace]
kubectl get pvc
kubectl get PV
kubectl Delete pv [pvName] -n [namespace]
kubectl get StorageClass

How to manage Dynamic storage class provisioner create nfs server, you can use git reference: https://github.com/sanjeev0407/Kubernates-Manifests-MithunTechnlogies/blob/master/pv-pvc/nfsstorageclass.yml

________________________________________________________________________________________________
________________________________________________________________________________________________
ConfigMap: A ConfigMap is an API object used to store non-confidential data in key-value pairs. 
           Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.
Caution: ConfigMap does not provide secrecy or encryption. 
         If the data you want to store are confidential, use a Secret rather than a ConfigMap, or use additional (third party) tools to keep your data private.

There are four different ways that you can use a ConfigMap to configure a container inside a Pod:
Inside a container command and args
Environment variables for a container
Add a file in read-only volume, for the application to read
Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap

kubectl get ConfigMap
kubectl exec configmap-demo-pod --ls /config

Secrets: Hashi-corp Valut
Types of Secret:
Opaque
Docker config secrets
Basic authentication Secret 
SSH authentication secrets
TLS secrets
Bootstrap token Secrets

echo "dev@!23" | base64  --> it's converting into the decoded value

Kubectl exec [PODName] -- ls
Kubectl exec [PODName] -- pwd
Kubectl exec [PODName] -- ls conf


How to pull private image from docker hub:
docker login -u <UserName> -P <Password> <Server>
kubectl create secret docker-registry <SecretName> --docker-server=<Dockerhub hostname FQDN > --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>

where:
<your-registry-server> is your Private Docker Registry FQDN. Use https://index.docker.io/v1/ for DockerHub.
<your-name> is your Docker username.
<your-pword> is your Docker password.
<your-email> is your Docker email.
You have successfully set your Docker credentials in the cluster as a Secret called regcred.

Probes(Health Check):
Suppose AWS ELB(Applcation Load Balancer) server health check fails?:
It's shifting trafic away from failure instance, the request is not sending to failure instance.

Liveness probes: Liveness probes to know when to restart a container.For example, liveness probes could catch a deadlock, where an application is running 
                 but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs.
Readiness probes: The kubelet uses readiness probes to know when a container is ready to start accepting traffic. A Pod is considered ready when all of its containers are ready. 
                 One use of this signal is to control which Pods are used as backends for Services. When a Pod is not ready, it is removed from Service load balancers.

200 ---> OK
201 ---> created
301 ---> Redirected
Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure.
_________________________________________________________________________________
________________________________________________________________________________________________
Stateful Sets:
StatefulSet is the workload API object used to manage stateful applications.
Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.

Stateful Deployments provide:
stable, unique network identifiers
Stable, persistent storage
ordered, graceful deployment and scaling
ordered, automated rolling updates

each pod having own storage
pvc will be created by statefulset
Services:
ClusterIp
NodePort
headless

StatefulSets currently require a Headless Service to be responsible for the network identity of the Pods. You are responsible for creating this Service.

Depends on number of replicas it's creating no.of PVC(persistentVolumeClaim)

MongoDB Port: 27017
mongod --replSet "rs0" --bind_ip [localhost,<hostname(s)|ip address(es)>]


kubectl exec -it [PodName] -n [NameSpace]
hostname -f
#enter into mongodb
mongosh
rs.status();
 
rs.initiate( {
   _id : "rs0",
   members: [
      { _id: 0, host: "mongodb0.example.net:27017" },
      { _id: 1, host: "mongodb1.example.net:27017" },
      { _id: 2, host: "mongodb2.example.net:27017" }
   ]
})

db.getSiblingDB("admin").createUser({user: "devdb", pwd role: "root", db: "admin"}]}); : "devdb123", roles: [{ role: "root", db: "admin"}]});


Question:
What is headless service in k8s:
Headless service is without loadbalancer; cluster ip, serviceIp's are not provided, Kube-proxy is not handle these Service but it's written all pod IP's

_________________________________________________________________________________
_________________________________________________________________________________
Scheduling:
Node selector
Node affinity
pod affinity  & Pod AntiAffinity
Taints & tolerations

Labels: It is case sensitive. Like many other Kubernetes objects, nodes have labels. You can attach labels manually.

kubectl get nodes --show-labels

kubectl label nodes <node-name> <label-key>=<label-value>
kubectl label nodes ip-10.21.32.1 name=workerone


Node Selector: nodeSelector is the simplest recommended form of node selection constraint. 
               You can add the nodeSelector field to your Pod specification and specify the node labels you want the target node to have. 
               Kubernetes only schedules the Pod onto nodes that have each of the labels you specify.
Node affinity: It is adavanced version of node selector. Affinity language is more expressive(we can use more logical operators in not in )
               Node affinity is conceptually similar to nodeSelector, allowing you to constrain which nodes your Pod can be scheduled on based on node labels. 
               There are two types of node affinity:

preferredDuringSchedulingIgnoredDuringExecution(Prefereed/Soft Rule): if it is not matching also ; The scheduler tries to find a node that meets the rule. 
                                                                      If a matching node is not available, the scheduler still schedules the Pod.
requiredDuringSchedulingIgnoredDuringExecution(required/Hard Rule): The scheduler can't schedule the Pod unless the rule is met. 
                                                                    This functions like nodeSelector, but with a more expressive syntax.

Weight: Matching multiple prefered rules for weight ; which one is high weightage pods running on that node


These are maintainence concepts:
Drain
Cordon
UnCordon

Taint:
Syntax: kubectl taint node [NodeName] [Key]=[Value]:Noschedule
kubectl taint node ip-172-31-38-173 node=Hatepods:Noschedule

If you want remove taints:
kubectl taint node ip-172-31-38-173 node=Hatepods:Noschedule-

kubectl describe node [NodeName]

For maintainence the Node:
kubectl cordon [NodeName] 

kubectl drain [NodeName] --ignore-daemonsets --delete-emptydir-data
Note: evicting(Shfting to another pod)pod
kubectl get pods -o wide 
kubectl get nodes

Once maintainence completed you can do uncordon:
kubectl uncordon [nodeIP]
____________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________
Network Policy:

If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster. 
NetworkPolicies are an application-centric construct which allow you to specify how a pod is allowed to communicate with various network "entities" (we use the word "entity" here to avoid overloading the more common terms such as "endpoints" and "services", which have specific Kubernetes connotations) over the network. 
NetworkPolicies apply to a connection with a pod on one or both ends, and are not relevant to other connections.

k8s pods communicate with Services  within cluster :
kubectl exec -it [PodName] -- bash
curl -v http://[ServiceName]
curl -v telnet://[Service.name].default.svc.cluster.local:80

The Two Sorts of Pod Isolation
There are two sorts of isolation for a pod: isolation for egress, and isolation for ingress.
Ingress(Incoming traffic):
Egress(Outgoing tarffic):

contorl the Ingress traffic follow by using network policy:

kubectl get networkpolicies -A 
kubectl get ns --show-labels  (Namespace labels will get)
kubectl get all --show-labels
kubectl label ns [NameSpace] name=test [creating Namespace]


Behavior of to and from selectors:

There are four kinds of selectors that can be specified in an ingress from section or egress to section:
podSelector: This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources or egress destinations.
namespaceSelector: This selects particular namespaces for which all Pods should be allowed as ingress sources or egress destinations.
namespaceSelector and podSelector: A single to/from entry that specifies both namespaceSelector and podSelector selects particular Pods within particular namespaces. 

Be careful to use correct YAML syntax.For example
  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
  ...
____________________________________________________________________________________________________________________ 
____________________________________________________________________________________________________________________
EKS(Elastic kubernetes Service-AWS)
AKS(Azure kubernetes Service)
GKS(google kubernetes Engine)
IKE (IBM kubernetes Engine)

KOPS(kubernetes operations): We can setup production eady highly available k8s cluster in AWS now KOPS is supporting Azure.

kops will not only help you create, destroy, upgrade and maintain production-grade, highly available, Kubernetes cluster, but it will also provision the necessary cloud infrastructure.

kOPS WILL levarage(use) AWS ASG(Auto scaling groups) Groups.
It will create 1ASG & It's  Launch template for masters
Another ASG & It's Launch template for workers
KOPS setup Reference: https://www.youtube.com/watch?v=79gN6j7Ub3k&ab_channel=MithunTechnologiesDevOps



EKS Cluster setUP:
Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. 
Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. Amazon EKS

Runs and scales the Kubernetes control plane across multiple AWS Availability Zones to ensure high availability.

Automatically scales control plane instances based on load, detects and replaces unhealthy control plane instances, and it provides automated version updates and patching for them.

Is integrated with many AWS services to provide scalability and security for your applications, including the following capabilities:
Amazon ECR for container images
Elastic Load Balancing for load distribution
IAM for authentication
Amazon VPC for isolation

Topics Covered:
1. Introduction to AWS EKS cluster
2. Prerequisites to create EKS.
3. Benefits of AWS EKS cluster
4. Different ways to setup EKS cluster. 
    AWS Management Console.
    Infra Structure As A code(Terraform). 
    eksctl utility provided by AWS.
5. Step by step procedure to setup EKS Cluster using Console.
6. Deploy Demo Application.

how to setup dedicated VPC by using cloudformation:
https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/amazon-eks-vpc-private-subnets.yaml

goto cloudformation and create stack and in that you can mention above url ; It's automatically creates VPC, subnets and route table


Cluster Autoscaler or Karpenter: by using adjusts the number of nodes in your cluster  when pods fail  or rescheduled onto other nodes.The cluster  Autoscaler uses Auto Scaling Groups.

reference: cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml
change cluster name:

________________________________________________________________________________________________________________
___________________________________________________________________________________________________________________
Ingress: Ingress is a collection of rule sthat allow inbound connections to reach the endpoints defined by a backend. 
An Ingress can be configured to give services externaly reachable urls, Load balancers tarffic, terminate SSL, offer name based virtual hosting etc.

k8s ingress is a resource to add rules for routing traffic from external sources to the services in the kubernetes cluster


kubectl explain ingress

git clone https://github.com/nginxinc/kubernetes-ingress.git --branch v3.1.1
cd kubernetes-ingress/deployments
kubectl apply -f common/ns-and-sa.yaml
kubectl apply -f rbac/rbac.yaml
kubectl apply -f ../examples/shared-examples/default-server-secret/default-server-secret.yaml
kubectl apply -f common/nginx-config.yaml
kubectl apply -f common/ingress-class.yaml
kubectl apply -f daemon-set/nginx-ingress.yaml
kubectl apply -f service/loadbalancer-aws-elb.yaml

Reference: https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/

________________________________________________________________________________________________________________
________________________________________________________________________________________________________________

How to edit & save the Windows Hosts file:
You can edit and save the host file, C:\Windows\System32\drivers\etc\hosts, in a few simple steps

Copy or drag the host file to your machine's desktop
Edit the file with notepad or any word processor of your choice

127.38.10.2 simplekubernetes.com
_____________________________________________________________________________________________________________
_____________________________________________________________________________________________________________

RBAC(Role Based Access Contorl): 
API Objects for configuring RBAC: 
Role, 
ClusterRole, 
RoleBinding, 
ClusterRoleBinding.

•	Role: This object is used to determine which operations can be carried out on which resources in a specific namespace.
•	ClusterRole: This object is used to determine which operations can be carried out on which resources anywhere in the cluster.
•	RoleBinding: This object is used to determine which users or service accounts are authorized to carry out operations on resources in a given namespace.
•	ClusterRoleBinding: This object is used to determine which users or service accounts are authorized to carry out operations on resources anywhere in the cluster.


authentication and authorization in kubernetes: ?
authentication and authorization are the building blocks of your cluster’s security.

Authentication:
Authentication (AuthN) is the process of validating or verifying that a user or entity is who (or what) they claim to be. In a security system, authentication must always come before authorization. 
There are many authentication processes, such as username and password combinations, one-time pins (OTPs), authentication apps with generated security codes, and biometric scans.

Authorization :
Authorization (AuthZ) is the step that follows the successful validation of a user or entity. The term authorization is synonymous with “access control”. 
Authorization refers to granting a validated user the appropriate permissions to carry out specific functions based on user rules and their roles in a system.

curl -v telnet://url:PORT    --> check connectivity

kubectl version --client

aws eks list-clusters

aws eks update-kubeconfig --name [EKS clusterName] --region [regionName]

cat ~/.kube/config

aws sts get-caller-identity 
aws get cm -n kube-system
CM: ConfigMap



___________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________

Resource Quota:

When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources.
Resource quotas are a tool for administrators to address this concern.

A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per namespace. 
It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources(CPU, Memory) that may be consumed by resources in that namespace.

Resource quotas work like this:

Different teams work in different namespaces. This can be enforced with RBAC.

The administrator creates one ResourceQuota for each namespace.

Users create resources (pods, services, etc.) in the namespace, and the quota system tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.

If creating or updating a resource violates a quota constraint, the request will fail with HTTP status code 403 FORBIDDEN with a message explaining the constraint that would have been violated.

If quota is enabled in a namespace for compute resources like cpu and memory, users must specify requests or limits for those values;
otherwise, the quota system may reject pod creation. Hint: Use the LimitRanger admission controller to force defaults for pods that make no compute resource requirements.



kubectl get quota
kubectl get quota -n [NameSpace]
kubectl get limitrange -n [Namespace]
kubectl delete quota [quotaName] -n [NameSpace]      ----> how to delete quota

____________________________________________________________________________________________________________________
____________________________________________________________________________________________________________________
Service:

kubectl get svc -n test-ns

In service yaml file not able to use labels; A service identifies pods by its LabelSelector.

ClusterIp: Expose the service on cluster-internal IP. Service is only reachable from within the cluster. This is the default type
           Service is just a logical concept, the real work is being done by the "Kube-proxy" pod that is running on each node
           It redirects requests from Cluster IP(Virtual IP address) to Pod IP
Kubectl describe service [ServiceName] -n test-ns
kubectl get all -o wide --show-lables -n test-ns

Inside container:
ls /etc/
cat /etc/resolv.conf
here: resolv.conf --> DNS server

how to show endpoints:
kubectl get ep 
ep: endpoint

FQDN: Full Qualified Domain name
In K8s FQDN format: <serviceName>.<nsName>.svc.cluster.local

ETCD: Kubernetes databasee

kubectl describe svc [serviceName]
_____________________________________________________________________________________________________________
Swap vs Swap memory?:
Swap is a space on the storage device (usually a hard disk or SSD) that the operating system uses as an extension of physical RAM (Random Access Memory). 
When the physical RAM becomes fully utilized, the operating system can move less frequently used data from RAM to the swap space to free up memory for other active processes.
The main purpose of using swap is to provide additional memory capacity when the physical RAM is exhausted.


K8s(Kubernetes):
kubectl version
kubectl get all

Namespace:
kubectl get ns or Namespace
kubectl create namespace dev
kubectl get all --namespace dev
kubectl create deployment sample-nginx --image=nginx  --replicas=3 --port=80 --namespace-dev
kubectl get [PODNAME] --namespace Dev

how to give particular access to dev, or QA team:

By using RBAC we need to give permissions for each team


Replication conroller(Replicaton set):
        
     Deployment
        |
     Replica-set
        |
       Pods
        |
     container
     
kubectl having two types 
... Imperative--> Adhoc commands --> No reuseful & can't trace it
... Declarative --> Manifest files(Yaml) -->Re-useable & Trace it easily

Kubectl delete [deploymentName]
Kubectl create deployment [deploymentName] --image=[Imagename:Tag]

kubectl api-resources   --> complete list of supported resources
kubectl api-resources | wc -l --> how many resources avaliable
kubectl explain <resource>  --> detailed description of that resource

kubectl get deployment or deployment
kubectl get replicaset
kubectl get pods
kubectl describe pod [PODname ]

kubectl scale deployment [deploymentName] --replicas=5

kubectl get pods -o wide

kubectl create deployment test-nginx --image=nginx:1.18-alphine --replicas=5

kubectl expose deployment test-nginx --type=LoadBalancer --port=8080

kubectl get svc

if you want any changes required 
kubectl edit deployment [depoymentName]

Kubernetes services Types:
1. ClusterIp
2. Node-port
3. LoadBalancer
4. ExternalName

kubectl get ds or DaemonSet
kubectl describe DaemonSet.apps/logging

What Is the Difference Between DaemonSet and Deployment? 
DaemonSet manages the number of pod copies to run in a node. Mointoing of workernodes
However, a deployment manages the number of pods and where they should be on nodes.Deployment selects nodes to place replicas using labels and other functions.

____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
persistenet volume and persistent volume claim (PV && PVC):
kubectl get PV
kubectl get PVC

in below yaml file you can mention Manual or dynamic; if you want any changes required in volume change in PVC yaml file

apiVersion: v1
kind: persistentVolumeClaim
metadata:
    name: sample-pv-claim
Spec:
    StorageClassName: manual
    accessmode:
        - ReadWriteOnce
    resources:
        requests:
            storage: 5Gi

fluentd it's mointoring tool



how create & run the yaml file:
nano sample-deploy.yaml
kubectl apply -f sample-deploy.yml
kubectl get all
kubectl describe pod [PODName ]

It's deleting all services , deployments and pods
kubectl delete all -all
kubectl get ingress -o wide
kubectl describe ingress nginx-rules
curl ipaddress


Mongo-config.yml:
---
apiVersion: v1
kind: ConfigMap
metadata:
    name: mongo-config
data:
    mongo-url: mongo-service
    mongo-port: 3000

Mongo-secret.Yaml:
---
apiVersion: v1
kind: Secret
metadata:
    name: Mongo-secret
type: Opaque
data:
    mongo-user: admin
    mongo-password: password
    
echo -n admin | base64
echo -n password | base64

kubectl get secrets
kubectl get ConfigMap
kubectl get statefulset
kubectl logs podname -all-containers=true
kubectl describe 

if you want increase the cpu increase and memory; it's cross the 128Mi, and cpu-500m
resources:
    requests:
        memory: "64Mi"
        cpu: "250m"
    limits:
        memory: "128Mi"
        cpu: "500m"

Node affinity:
Container size increases:

______________________________________________________________________________________________________
Helm having ArtifactHub ; it's just like dockerHub; it's storing the helm charts

Helm:
Helm is a tool that automates the creation, packaging, configuration, and deployment of Kubernetes applications by 
combining your configuration files into a single reusable package

Docker Hub vs Helm: What are the differences?
Docker Hub: Developers describe Docker Hub as "Build and Ship any Application Anywhere". It is the world's easiest way to create, manage, and deliver your teams' container applications. 
It is the perfect home for your teams' applications.
Helm: Helm is detailed as "The Kubernetes Package Manager". Helm is the best way to find, share, and use software built for Kubernetes.
ArtifactHub: By using artifact hub we need to download helm charts
helm repo ls



brew install Helm
brew upgrade Helm


#if you want see branches wise of directoy you can install tree
yum install tree -y

helm version
helm repo list
helm list or ls

if you want delete repo in Helm: helm repo remove bitmani[RepoNam]
if you want add repo in Helm: helm repo add [] [url]

helm create [ChartName Or Directory]  -->In directory you will findout another templates directory --> In that you will get deployment, ingress, serviceaccount yaml files

helm history [release]

helm search repo [RepoName]
helm ls -n kube-system
helm show values [directory]
helm template [directory]  --> it's updating the helm charts locally 
default values are ther; if you want enable you can do
helm upgrade --install [releaseName] [DirectoryName] --set autoscalling.enabled=true --dry-run
helm upgrade --install javaapp javawebapp --set autoscalling.enabled=true --dry-run

helm upgrade --install [releaseName][DirectoryName] -n test-ns
helm upgrade --install [releaseName][DirectoryName] --set image.tag=4 -n test-ns
helm ls -n test-ns

#rollback previous version of applicaiton
helm rollback [releaseName] -n test-ns

#if you want uninstall every version 
helm uninstall [releaseName] -n test-ns

# if you want tar package
helm package [DirectoryName]

#if you want change run time variables use this Commands
helm upgrade --install javaapp javawebapp-0.1.0.tgz -f prodvalues.yaml --set image.tag=7 -n test-ns
helm upgrade --install javaapp javawebapp-0.1.0.tgz  --set image.tag=6 -n test-ns

helm get clusterrole -n kube-system
kubectl top Nodes
_____________________________________________________________________________________________________________
_____________________________________________________________________________________________________________
how to add gmail is SMPT

login to gmail --> Accounts settings --> security --> enable 2-step verification --> get started
After enabling the 2 step verification --> go to 2 step verfication --> click on apps password --> create with name(Others) --> we will get one password
________________________________________________________________________________________________________________
Istio sidecar  --> Envoy Proxy

IF you want see nano file line number:
:se nu

_____________________________________________________________________________________________________________
_____________________________________________________________________________________________________________

Agenda: Multi Master K8's Cluster Setup Using Kubeadm In AWS EC2 Ubuntu Servers:

What is Kubeadm?
Kubeadm is a tool built to provide kubeadm init and kubeadm join as best-practice for creating Kubernetes clusters. kubeadm performs the actions necessary to get a cluster up and running.
Pre-requisite
For this demo, we will use 3 master and 2 worker node to create a multi master kubernetes cluster using kubeadm installation tool. Below are the pre-requisite requirements for the installation:
3 machines for master, ubuntu 20.04+, 2 CPU, 4 GB RAM, 10 GB storage
2 machines for worker, ubuntu 20.04+, 1 CPU, 1 GB RAM, 10 GB storage
1 machine for loadbalancer, ubuntu 20.04+, 1 CPU, 1 GB RAM, 10 GB storage

All machines must be accessible on the network. For cloud users - single VPC for all machines
sudo privilege.

Setting up loadbalancer:
In order to setup loadbalancer, you can leverage any loadbalancer utility of your choice. If you want to use cloud based TCP loadbalancer,feel free to use. There is no restriction regarding the tool you want to use for this purpose. For our demo, we will use HAPROXY
as the primary loadbalancer.

What are we loadbalancing?
We have 3 master nodes. Which means the user can connect to either of the 3 api-servers. The loadbalancer will be used to loadbalance between the 3 api-servers.


First Master Machine only need to kubeadm init; don't run on multi master machines




